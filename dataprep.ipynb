{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_q = \"\"\"{\"phase\":1,\"question\":\"Find the Distinct count of games for RPG genre\",\"sql\":{\"conds\":[[8,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"how many distinct games for RPG genre\",\"sql\":{\"conds\":[[8,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"find total games for FPA genre\",\"sql\":{\"conds\":[[8,0,\"FPA\"]],\"sel\":4,\"agg\":3},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"find distinct number of games for RPG genre\",\"sql\":{\"conds\":[[8,0,\"RPG\"]],\"sel\":4,\"agg\":6},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"FIND TOTAL primary playtime in seconds FOR GAME_NAME_ID = 123456\",\"sql\":{\"conds\":[[3,0,\"123456\"]],\"sel\":23,\"agg\":4},\"table_id\":\"1-9132138-1\"}\n",
    "{\"phase\":1,\"question\":\"FIND TOTAL NUMBER OF ACCOUNTS WHO PLAYED ON DATE = 11/20/2018\",\"sql\":{\"conds\":[[1,0,\"11/20/2018\"]],\"sel\":20,\"agg\":3},\"table_id\":\"1-9132138-1\"}\n",
    "{\"phase\":1,\"question\":\"Find the Number of games under franchise 125690\",\"sql\":{\"conds\":[[2,0,\"125690\"]],\"sel\":4,\"agg\":3},\"table_id\":\"1-9132139-1\"}\n",
    "{\"phase\":1,\"question\":\"Find the number of distinct Title_ID for Horizon Zero Dawn\",\"sql\":{\"conds\":[[4,0,\"Horizon Zero Dawn\"]],\"sel\":0,\"agg\":6},\"table_id\":\"1-9132139-1\"}\"\"\".split('\\n')\n",
    "extra_col = \"\"\"{\"id\":\"1-9132139-1\",\"header\":['TITLE_ID','FRANCHISE_NAME_ID','FRANCHISE_NAME','GAME_NAME_ID','GAME_NAME','SUPER_TITLE_NAME_ID','SUPER_TITLE_NAME','GAME_GENRE_ID','GAME_GENRE','PARTY_TYPE_ID','PARTY_TYPE','GLOBAL_SERVICE_PROVIDER_ID','GLOBAL_SERVICE_PROVIDER_ID_ID','GLOBAL_SERVICE_PROVIDER_NAME','GLOBAL_TITLE_ID','GLOBAL_TITLE_ID_ID','GLOBAL_TITLE_NAME','PMT_SERVICE_PROVIDER_ID_ID','PMT_SERVICE_PROVIDER_ID','PMT_SERVICE_PROVIDER_NAME','PMT_SERVICE_PROVIDER_JP_NAME','TITLE_NAME','SALES_TITLE_ID','SALES_TITLE','SALES_FRANCHISE_ID','SALES_FRANCHISE','PMT_TITLE_NAME','PMT_TITLE_JP_NAME','DISK_TITLE_NAME','TMDB_TITLE_NAME','ADOBE_RECOMMENDATIONS_FRANCHISE_NAME_ID','ADOBE_RECOMMENDATIONS_FRANCHISE_NAME','ADOBE_RECOMMENDATIONS_TITLE_NAME_ID','ADOBE_RECOMMENDATIONS_TITLE_NAME','TITLE_TYPE','TITLE_TYPE_ID','TITLE_TYPE_LEVEL_2','TITLE_TYPE_LEVEL_2_ID','TITLE_TYPE_LEVEL_3','TITLE_TYPE_LEVEL_3_ID','PLUGIN_IND','PLUGIN_GROUP_ID','PLUGIN_GROUP','MOVE_EXCLUSIVE_IND','TITLE_ATTRIBUTE_OVERRIDE_SET','SCEA_TITLE_RELEASE_RHQ_DT','SCEE_TITLE_RELEASE_RHQ_DT','SCEJ_TITLE_RELEASE_RHQ_DT','SCEASIA_TITLE_RELEASE_RHQ_DT','SCEA_GAME_RELEASE_RHQ_DT','SCEE_GAME_RELEASE_RHQ_DT','SCEJ_GAME_RELEASE_RHQ_DT','SCEASIA_GAME_RELEASE_RHQ_DT','PMT_TITLE_NAME_LONG','PMT_TITLE_NAME_ORIGINAL','PMT_TITLE_NAME_CLEANSED_IND','UNITY_TITLE_NAME','UNITY_TITLE_NAME_ID','UNITY_PUBLISHING_MODEL','UNITY_PUBLISHING_MODEL_ID','TITLE_PUBLISHER_TYPE','TITLE_DISTRIBUTION_REGION','TITLE_TARGET_CONSOLE_GENERATION','TITLE_ORIGINAL_CONSOLE_GENERATION','TITLE_MEDIA_TYPE','GAME_MEDIA_TYPE','TITLE_CONTENT_SUB_TYPE','TITLE_CONTENT_TYPE','MULTI_DISC_IND','SOURCE_TITLE_ID','SOURCE_SYSTEM_ID','ETL_ID_INSERTED','ETL_ID_UPDATED']}\n",
    "{\"id\":\"1-9132138-1\",\"header\":['GAME_PLAY_SESSION_ID','SESSION_START_UTC_DTTM','SESSION_START_UTC_DT_ID','SESSION_START_RHQ_DT_ID','SESSION_START_LOCATION_DT_ID','SESSION_START_UTC_TIME_ID','SESSION_START_LOCATION_TIME_ID','DEVICE_ID','DEVICE_TYPE_ID','IP_GEO_LOCATION_ID','SESSION_START_ACCT_ID','SESSION_START_ACCT_SCE_REGION_ID','SESSION_START_ACCT_COUNTRY_ISO_CODE','TITLE_ID','MEDIA_ID','SESSION_TYPE_ID','APPLICATION_VERSION_ID','FIRMWARE_VERSION_ID','GAME_PLAY_SESSION_CHARACTERISTICS_ID','GAME_PLAY_CHARACTERISTICS_ID','PRIMARY_ACCT_ID','PRIMARY_ACCT_SCE_REGION_ID','PRIMARY_ACCT_COUNTRY_ISO_CODE','PRIMARY_SESSION_LENGTH_SECONDS','SESSION_LENGTH_SECONDS','PSN_ACCOUNTS','LOCAL_ACCOUNTS','CONNECTED_DS4S','SECONDARY_GAMEPLAY_IND','LOCAL_GAMEPLAY_IND','SESSION_LENGTH_DEFAULTED_IND','SOURCE_SYSTEM_ID','ETL_ID_INSERTED']}\"\"\".replace(\"\\'\", '\\\"').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.loads(extra_col[0])['header'][8]#.index('FRANCHISE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import re\n",
    "import json, pandas as pd\n",
    "import numpy as np\n",
    "from lib.query import Query\n",
    "import torch\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score \n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow\n",
    "import keras\n",
    "import re\n",
    "import json, pandas as pd\n",
    "import numpy as np\n",
    "# from lib.query import Query\n",
    "import torch\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nl2sql.dataloader import DataTransformer\n",
    "import os\n",
    "# os.chdir('/home/project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query.agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG', 'COUNT_DISTINCT']\n",
    "class load_data:\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "\n",
    "def inject_table_names(sql, table_cols):\n",
    "    \"\"\"\n",
    "    replace col<index> in `sql` query with the <column_name> from `table_cols`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        regex = re.compile(r'\\scol(\\d+)')\n",
    "        return regex.sub(lambda x: ' ' + table_cols[int(x.string[x.start()+4: x.end()])],  str(sql))\n",
    "    except Exception as e:\n",
    "        print(e, sql, table_cols)\n",
    "#         pass\n",
    "    return None\n",
    "# def clean_text(text):\n",
    "#     return \n",
    "#     text = re.sub('\\W+', ' ', text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filepath_phrase, filepath_table):\n",
    "    \n",
    "    lines = open(filepath_phrase, 'r').readlines()\n",
    "    lines += extra_q\n",
    "    df_phrases = pd.DataFrame([json.loads(line) for line in lines])\n",
    "    lines = open(filepath_table, 'r').readlines()\n",
    "    lines +=extra_col\n",
    "    df_tables = pd.DataFrame([json.loads(line) for line in lines])\n",
    "    # join table data with sql data\n",
    "    df = pd.merge(df_phrases, df_tables[['id', 'header']], left_on='table_id', right_on='id')\n",
    "#     df= df[:10000]\n",
    "    df['query_temp'] = df.sql.apply(lambda data: Query.from_dict(d=data))\n",
    "    df['query'] = df.loc[:, ['query_temp', 'header']]\\\n",
    "        .apply(lambda row: inject_table_names(row[0], row[1]), axis=1)\n",
    "    df['agg'] = df['sql'].apply(lambda x: x['agg'])\n",
    "    df['sel_col'] = df['sql'].apply(lambda x: x['sel'])\n",
    "    \n",
    "#     questions = df['question'].apply(clean_text).values\n",
    "    questions = df['question'].values\n",
    "    queries = df['query'].values\n",
    "    column_names = df['header'].values\n",
    "    agg = df['agg'].values\n",
    "    sel_col = df['sel_col'].values\n",
    "    return questions, queries, column_names, df, agg, sel_col \n",
    "\n",
    "train_questions, train_queries, train_column_names, df_train, train_agg, train_sel_col,  = load_data('../data/train.jsonl', '../data/train.tables.jsonl')\n",
    "test_questions, test_queries, test_column_names, df_test, test_agg, test_sel_col = load_data('../data/test.jsonl', '../data/test.tables.jsonl')\n",
    "dev_questions, dev_queries, dev_column_names, _, dev_agg, dev_sel_col = load_data('../data/dev.jsonl', '../data/dev.tables.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train[:1000]\n",
    "# df_test = df_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG', 'COUNT_DISTINCT'],\n",
       " ['=', '>', '<', 'OP'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_ops = Query.agg_ops\n",
    "cond_ops = Query.cond_ops\n",
    "len(agg_ops)\n",
    "agg_ops, cond_ops\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Not Found in Embedding Matrix:  UNKWORD\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "datatransformer = DataTransformer(df_train['question'].values, df_train['header'].values, df_train['sql'].values, cond_ops, agg_ops, embedding_filepath='../data/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_label = datatransformer.reverse_label_sequence(train_target, questions=df_train['question'].values)\n",
    "# reverse_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sony_df = pd.read_csv('nl2sql/Sony_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = datatransformer.transform_questions(df_train['question'].values)\n",
    "train_columns_sequences = datatransformer.transform_columns(df_train['header'].values)\n",
    "train_target = datatransformer.label_sequence(df_train['sql'].values, df_train['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.loc[1, 'header'], datatransformer.tokenizer.sequences_to_texts(train_columns_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = datatransformer.transform_questions(df_test['question'].values)\n",
    "test_columns_sequences = datatransformer.transform_columns(df_test['header'].values)\n",
    "test_target = datatransformer.label_sequence(df_test['sql'].values, df_test['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# train_sequences, train_columns_sequences, train_target = pickle.load(open('data/train_data.pic', 'rb'))\n",
    "pickle.dump([train_sequences, train_columns_sequences, train_target], open('data/train_data.pic', 'wb'))\n",
    "# test_sequences, test_columns_sequences, test_target = pickle.load(open('data/test_data.pic', 'rb'))\n",
    "pickle.dump([test_sequences, test_columns_sequences, test_target], open('data/test_data.pic', 'wb'))\n",
    "# import pickle\n",
    "# datatransformer = pickle.load(open('data/datatransformerlemm.pic', 'rb'))\n",
    "pickle.dump(datatransformer, open('data/datatransformerlemm.pic', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_agg_ohe, test_column_ohe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
